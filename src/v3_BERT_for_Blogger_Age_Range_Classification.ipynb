{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v3 BERT for Blogger Age Range Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sz6snE6uNqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (DataLoader, RandomSampler, WeightedRandomSampler, SequentialSampler, TensorDataset)\n",
        "\n",
        "!pip install pytorch_pretrained_bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.modeling import BertLayerNorm\n",
        "\n",
        "\"\"\"\n",
        "!pip install snorkel\n",
        "import snorkel\n",
        "from snorkel.classification import cross_entropy_with_probs\n",
        "\"\"\"\n",
        "\n",
        "USE_DRIVE = True\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive, files\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_FOLDER = '/content/drive/My Drive/Colab Notebooks/data/blogger_age_range/'\n",
        "    OUT_FOLDER = '/content/drive/My Drive/Colab Notebooks/out/blogger_age_range/'\n",
        "else:\n",
        "    DATA_FOLDER = '/data/'\n",
        "    OUT_FOLDER = '/out/'\n",
        "\n",
        "SAVED_MODELS_FOLDER = 'models/'\n",
        "SUBMISSIONS_FOLDER = 'submissions/'\n",
        "!mkdir -p \"{OUT_FOLDER}{SAVED_MODELS_FOLDER}\"\n",
        "!mkdir -p \"{OUT_FOLDER}{SUBMISSIONS_FOLDER}\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5D-DEC5zxLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BLOG = 'blog'\n",
        "CLASS = 'class'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHdCJyTY1fuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filename, folder='./data/', has_labels=False):\n",
        "    path = folder + filename\n",
        "    dataset = pd.read_csv(path, names=[BLOG, CLASS])\n",
        "    if has_labels:\n",
        "        return dataset[BLOG].to_numpy(), dataset[CLASS].to_numpy()\n",
        "    else:\n",
        "        return dataset[BLOG].to_numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvN6yDD9DVHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(data, labels, valid_prop, test_prop=0, seed=1234):\n",
        "    \"\"\"\n",
        "    Function that takes a dataset and splits it into three subsets: a training set, a validation set, and a test set\n",
        "    :param dataset: Complete dataset to split into training validation and test sets\n",
        "    :param valid_prop: What proportion (in percentage; expressed as a value from 0 to 1) of the full dataset should be\n",
        "    used for the validation set\n",
        "    :param test_prop: What proportion (in percentage; expressed as a value from 0 to 1) of the full dataset should be\n",
        "    used for the test set\n",
        "    :return: A tuple containing, in that order:\n",
        "        * The datapoints of the training set\n",
        "        * The label of each datapoint in the training set\n",
        "        * The datapoints of the validation set\n",
        "        * The label of each datapoint in the validation set\n",
        "        * The datapoints of the test set\n",
        "        * The label of each datapoint in the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # we set the seed for reproducibility\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    # shuffle data according to seed\n",
        "    idx = np.arange(data.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    data = data[idx]\n",
        "    labels = labels[idx]\n",
        "\n",
        "    # retrieve classes\n",
        "    classes = list(set(labels))\n",
        "    classes.sort()\n",
        "\n",
        "    # will be needed to select indices for train/valid/test splits\n",
        "    idx = np.arange(data.shape[0])\n",
        "    train_idx, valid_idx, test_idx = [], [], []\n",
        "\n",
        "    # get split indices for every class\n",
        "    for i in classes:\n",
        "        class_idx = idx[labels == i]\n",
        "        class_idx = np.array_split(class_idx, 1 / DATASET_FRACTION)[0]\n",
        "\n",
        "        n = len(class_idx)\n",
        "        valid_split_idx = int(valid_prop * n)\n",
        "        test_split_idx = int((1 - test_prop) * n)\n",
        "\n",
        "        train_idx.append(class_idx[valid_split_idx:test_split_idx])\n",
        "        valid_idx.append(class_idx[:valid_split_idx])\n",
        "        test_idx.append(class_idx[test_split_idx:])\n",
        "\n",
        "    # find class with less examples\n",
        "    train_class_counts = np.asarray(list(map(len, train_idx)))\n",
        "    train_class_weights = 1 / len(classes) / train_class_counts\n",
        "\n",
        "    # concatenate indices of all classes\n",
        "    train_idx = np.concatenate([arr for arr in train_idx])\n",
        "    valid_idx = np.concatenate([arr for arr in valid_idx])\n",
        "    test_idx = np.concatenate([arr for arr in test_idx])\n",
        "\n",
        "    train_examples_weights = train_class_weights / len(train_idx)\n",
        "    train_idx_weights = [train_examples_weights[i]\n",
        "                            for i, count in enumerate(train_class_counts)\n",
        "                                for _ in range(count)]\n",
        "\n",
        "    # shuffle the datasets\n",
        "    #np.random.shuffle(train_idx)\n",
        "    np.random.shuffle(valid_idx)\n",
        "    np.random.shuffle(test_idx)\n",
        "    train_data, train_labels = data[train_idx], labels[train_idx]\n",
        "    valid_data, valid_labels = data[valid_idx], labels[valid_idx]\n",
        "    test_data, test_labels = data[test_idx], labels[test_idx]\n",
        "    \n",
        "    if test_prop is 0:\n",
        "        return train_data, train_labels, valid_data, valid_labels, train_idx_weights\n",
        "    else:\n",
        "        return train_data, train_labels, valid_data, valid_labels, test_data, test_labels, train_idx_weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V33YBiPClng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(tokenizer, raw_data, labels=None, train_idx_weights=None):\n",
        "\n",
        "    all_inputs = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, data in enumerate(tqdm(raw_data)):\n",
        "\n",
        "        tokenized_data = tokenizer.tokenize(data)\n",
        "\n",
        "        if len(tokenized_data) > MAX_LENGTH - 2:\n",
        "            tokenized_data = tokenized_data[:(MAX_LENGTH - 2)]\n",
        "\n",
        "        tokenized_data = [\"[CLS]\"] + tokenized_data + [\"[SEP]\"]\n",
        "        tokenized_len = len(tokenized_data)\n",
        "\n",
        "        padding = [0] * (MAX_LENGTH - tokenized_len)\n",
        "\n",
        "        all_inputs.append(tokenizer.convert_tokens_to_ids(tokenized_data) + padding)\n",
        "        all_masks.append([1]*tokenized_len + padding)\n",
        "        all_segments.append([0]*tokenized_len + padding)\n",
        "        if labels is not None:\n",
        "            all_labels.append(labels[i])\n",
        "\n",
        "    all_inputs = torch.tensor(all_inputs)\n",
        "    all_masks = torch.tensor(all_masks)\n",
        "    all_segments = torch.tensor(all_segments)\n",
        "    all_labels = torch.tensor(all_labels)\n",
        "    \n",
        "    # wrap inputs in a dataloader\n",
        "    if labels is not None:\n",
        "        dataset = TensorDataset(all_inputs, all_masks, all_segments, all_labels)\n",
        "        sampler = WeightedRandomSampler(train_idx_weights, len(train_idx_weights), replacement=True)\n",
        "        dataloader = DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        dataset = TensorDataset(all_inputs, all_masks, all_segments)\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return dataloader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__UkT6Xn26oX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, dataloader):\n",
        "    model.eval()\n",
        "    preds = torch.zeros(len(dataloader.dataset))\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
        "            # send examples to the device and extract them from tuples\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs, masks, segments = batch\n",
        "\n",
        "            # forward pass and predictions\n",
        "            logits = model(inputs, masks, segments)\n",
        "            preds[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE] = logits.argmax(dim=1)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgRXeyXJ2bIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load datasets\n",
        "training_data, training_labels = load_data('data_train_preprocessed.csv', folder=DATA_FOLDER, has_labels=True)\n",
        "submission_data = load_data('data_test_preprocessed.csv', folder=DATA_FOLDER)\n",
        "\n",
        "# extract classes\n",
        "classes = list(set(training_labels))\n",
        "classes.sort()\n",
        "class_dict = {label: i for i, label in enumerate(classes)}\n",
        "num_classes = len(class_dict)\n",
        "\n",
        "# convert labels to integers\n",
        "training_labels = np.asarray(list(map(lambda label: class_dict[label], training_labels)))\n",
        "\n",
        "print(class_dict)\n",
        "print(num_classes)\n",
        "print()\n",
        "print(len(training_data))\n",
        "print(len(training_labels))\n",
        "print(len(submission_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_Z16LCqzg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommanded hyperparameters:\n",
        "# Batch size: 16, 32\n",
        "# Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "# Number of epochs: 2, 3, 4\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 4\n",
        "\n",
        "BERT_KIND = 'bert-base-uncased'\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "DATASET_FRACTION = 1  # fraction of training examples to use (between 0 and 1)\n",
        "\n",
        "# set seeds for reproducibility\n",
        "SEED = 1234\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# split dataset\n",
        "train_data, train_labels, valid_data, valid_labels, train_idx_weights = \\\n",
        "    split_data(training_data, training_labels, valid_prop=.1, seed=SEED)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print()\n",
        "\n",
        "# Bert tokenizer instantiation\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_KIND)\n",
        "\n",
        "# prepare data for Bert Sequence Classification\n",
        "train_dataloader = prepare_data(tokenizer, train_data, train_labels, train_idx_weights)\n",
        "valid_dataloader = prepare_data(tokenizer, valid_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V42TkvwMDwpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained(BERT_KIND, num_labels=num_classes).to(device)\n",
        "\n",
        "# optimizer configuration\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# instantiate optimizer\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=LEARNING_RATE,\n",
        "                     warmup=.1,\n",
        "                     t_total=(int(len(train_dataloader.dataset) / BATCH_SIZE) * NUM_EPOCHS))\n",
        "\n",
        "# instantiate loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#criterion = cross_entropy_with_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ekB7Ny0aDfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # send examples to the device and extract them from tuples\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, segments, labels = batch\n",
        "\n",
        "        # forward pass\n",
        "        logits = model(inputs, masks, segments)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # reset gradients and accumulate loss\n",
        "        optimizer.zero_grad()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # compute validation accuracy\n",
        "    preds = predict(model, valid_dataloader)\n",
        "    accuracy = np.mean(preds.numpy() == valid_labels)\n",
        "\n",
        "    print(f'\\nEpoch {epoch+1}: loss={epoch_loss}')\n",
        "    print(f'Validation accuracy = {accuracy}\\n')\n",
        "\n",
        "    # save model if best accuracy yet\n",
        "    if accuracy > best_accuracy:\n",
        "        model_save_name = f'model_{LEARNING_RATE}_{MAX_LENGTH}_{BATCH_SIZE}_{NUM_EPOCHS}_{accuracy}.pt'\n",
        "        path = OUT_FOLDER + SAVED_MODELS_FOLDER + model_save_name\n",
        "        torch.save(model.state_dict(), path)\n",
        "        best_accuracy = accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6nfISjmNacx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "LEARNING_RATE = \n",
        "MAX_LENGTH = \n",
        "BATCH_SIZE = \n",
        "NUM_EPOCHS = \n",
        "best_accuracy = \n",
        "model_save_name = f'model_{LEARNING_RATE}_{MAX_LENGTH}_{BATCH_SIZE}_{NUM_EPOCHS}_{best_accuracy}.pt'\n",
        "path = OUT_FOLDER + SAVED_MODELS_FOLDER + model_save_name\n",
        "\"\"\"\n",
        "\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "submission_dataloader = prepare_data(tokenizer, submission_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyUnD63WB7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = predict(model, submission_dataloader)\n",
        "preds_classes = np.asarray(list(map(lambda pred: classes[pred], preds.long())))\n",
        "\n",
        "filename = f'submission_{LEARNING_RATE}_{MAX_LENGTH}_{BATCH_SIZE}_{NUM_EPOCHS}_{best_accuracy}.csv'\n",
        "path = OUT_FOLDER + SUBMISSIONS_FOLDER + filename\n",
        "\n",
        "with open(path, 'w+') as file:\n",
        "    file.write('Id,Category')\n",
        "    for i, pred in enumerate(preds_classes):\n",
        "        file.write(f'\\n{i},{pred}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tih3g7Lui1bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}