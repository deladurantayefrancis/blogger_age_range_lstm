{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM for Blogger Age Range Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi54e07FX_x_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import (DataLoader, RandomSampler, WeightedRandomSampler, SequentialSampler, TensorDataset)\n",
        "\n",
        "!pip install snorkel\n",
        "import snorkel\n",
        "from snorkel.classification import cross_entropy_with_probs\n",
        "\n",
        "USE_DRIVE = True\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive, files\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_FOLDER = '/content/drive/My Drive/Colab Notebooks/data/blogger_age_range/'\n",
        "    OUT_FOLDER = '/content/drive/My Drive/Colab Notebooks/out/blogger_age_range/'\n",
        "else:\n",
        "    DATA_FOLDER = 'data/'\n",
        "    OUT_FOLDER = 'out/'\n",
        "\n",
        "SAVED_MODELS_FOLDER = 'models/'\n",
        "SUBMISSIONS_FOLDER = 'submissions/'\n",
        "!mkdir -p \"{OUT_FOLDER}{SAVED_MODELS_FOLDER}\"\n",
        "!mkdir -p \"{OUT_FOLDER}{SUBMISSIONS_FOLDER}\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-cY7Lo9saZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BLOG = 'blog'\n",
        "CLASS = 'class'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBVhD373cCMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filename, folder='./data/', has_labels=False):\n",
        "    path = folder + filename\n",
        "    dataset = pd.read_csv(path, names=[BLOG, CLASS])\n",
        "    if has_labels:\n",
        "        return dataset[BLOG].to_numpy(), dataset[CLASS].to_numpy()\n",
        "    else:\n",
        "        return dataset[BLOG].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWyVgjl2YDSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(data, labels, valid_prop, test_prop=0, seed=1234):\n",
        "    \"\"\"\n",
        "    Function that takes a dataset and splits it into three subsets: a training set, a validation set, and a test set\n",
        "    :param dataset: Complete dataset to split into training validation and test sets\n",
        "    :param valid_prop: What proportion (in percentage; expressed as a value from 0 to 1) of the full dataset should be\n",
        "    used for the validation set\n",
        "    :param test_prop: What proportion (in percentage; expressed as a value from 0 to 1) of the full dataset should be\n",
        "    used for the test set\n",
        "    :return: A tuple containing, in that order:\n",
        "        * The datapoints of the training set\n",
        "        * The label of each datapoint in the training set\n",
        "        * The datapoints of the validation set\n",
        "        * The label of each datapoint in the validation set\n",
        "        * The datapoints of the test set\n",
        "        * The label of each datapoint in the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # we set the seed for reproducibility\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    # shuffle data according to seed\n",
        "    idx = np.arange(data.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    data = data[idx]\n",
        "    labels = labels[idx]\n",
        "\n",
        "    # retrieve classes\n",
        "    classes = list(set(labels))\n",
        "    classes.sort()\n",
        "\n",
        "    # will be needed to select indices for train/valid/test splits\n",
        "    idx = np.arange(data.shape[0])\n",
        "    train_idx, valid_idx, test_idx = [], [], []\n",
        "\n",
        "    # get split indices for every class\n",
        "    for i in classes:\n",
        "        class_idx = idx[labels == i]\n",
        "        class_idx = np.array_split(class_idx, 1 / DATASET_FRACTION)[0]\n",
        "\n",
        "        n = len(class_idx)\n",
        "        valid_split_idx = int(valid_prop * n)\n",
        "        test_split_idx = int((1 - test_prop) * n)\n",
        "\n",
        "        train_idx.append(class_idx[valid_split_idx:test_split_idx])\n",
        "        valid_idx.append(class_idx[:valid_split_idx])\n",
        "        test_idx.append(class_idx[test_split_idx:])\n",
        "\n",
        "    # find class with less examples\n",
        "    train_class_counts = np.asarray(list(map(len, train_idx)))\n",
        "    train_class_weights = 1 / len(classes) / train_class_counts\n",
        "\n",
        "    # concatenate indices of all classes\n",
        "    train_idx = np.concatenate([arr for arr in train_idx])\n",
        "    valid_idx = np.concatenate([arr for arr in valid_idx])\n",
        "    test_idx = np.concatenate([arr for arr in test_idx])\n",
        "\n",
        "    train_examples_weights = train_class_weights / len(train_idx)\n",
        "    train_idx_weights = [train_examples_weights[i]\n",
        "                            for i, count in enumerate(train_class_counts)\n",
        "                                for _ in range(count)]\n",
        "\n",
        "    # shuffle the datasets\n",
        "    #np.random.shuffle(train_idx)\n",
        "    np.random.shuffle(valid_idx)\n",
        "    np.random.shuffle(test_idx)\n",
        "    train_data, train_labels = data[train_idx], labels[train_idx]\n",
        "    valid_data, valid_labels = data[valid_idx], labels[valid_idx]\n",
        "    test_data, test_labels = data[test_idx], labels[test_idx]\n",
        "    \n",
        "    if test_prop is 0:\n",
        "        return train_data, train_labels, valid_data, valid_labels, train_idx_weights\n",
        "    else:\n",
        "        return train_data, train_labels, valid_data, valid_labels, test_data, test_labels, train_idx_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCDQD6cu0674",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(tokenizer, raw_data, labels=None, train_idx_weights=None):\n",
        "\n",
        "    all_inputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, data in enumerate(tqdm(raw_data)):\n",
        "\n",
        "        # split input into words (tokens)\n",
        "        words = data.split()\n",
        "\n",
        "        # truncate input if longer than max length\n",
        "        if len(words) > MAX_LENGTH:\n",
        "            words = words[:MAX_LENGTH]\n",
        "\n",
        "        # compute padding length\n",
        "        padding = [0] * (MAX_LENGTH - len(words))\n",
        "\n",
        "        # add padding to and store the input\n",
        "        all_inputs.append(tokenizer.convert_words_to_ids(words) + padding)\n",
        "        if labels is not None:\n",
        "            all_labels.append(labels[i])\n",
        "\n",
        "    # convert inputs to torch tensors\n",
        "    all_inputs = torch.tensor(all_inputs)\n",
        "    all_labels = torch.tensor(all_labels)\n",
        "    \n",
        "    # wrap inputs in a dataloader\n",
        "    if labels is not None:\n",
        "        dataset = TensorDataset(all_inputs, all_labels)\n",
        "        sampler = WeightedRandomSampler(train_idx_weights, len(train_idx_weights), replacement=True)\n",
        "        dataloader = DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        dataset = TensorDataset(all_inputs)\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgopUiYM14rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tokenizer():\n",
        "\n",
        "    def __init__(self, sentences, threshold=1):\n",
        "\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # get full list of words\n",
        "        all_words = ' '.join(sentences).split()\n",
        "\n",
        "        # if there's a threshold, remove words less frequent than threshold\n",
        "        if threshold > 1:\n",
        "            vocabulary = Counter(all_words)\n",
        "            vocabulary = [word for word, freq in vocabulary.items() if freq >= threshold]\n",
        "        else:\n",
        "            vocabulary = list(set(all_words))\n",
        "\n",
        "        # adding 'padding' and 'unknown' tokens to vocabulary\n",
        "        vocabulary = ['PAD', 'UNK'] + vocabulary\n",
        "        \n",
        "        # create words to ids dictionary\n",
        "        self.tokens_to_ids = {tok: i for i, tok in enumerate(tqdm(vocabulary))}\n",
        "\n",
        "    def convert_words_to_ids(self, tokens):\n",
        "\n",
        "        # build list containing the ids of the tokens received as input\n",
        "        ids = [self.tokens_to_ids[tok]\n",
        "                if tok in self.tokens_to_ids else self.tokens_to_ids['UNK'] for tok in tokens]\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoTd2A2KbgyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, dataloader):\n",
        "    model.eval()\n",
        "    preds = torch.zeros(len(dataloader.dataset))\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
        "            # send examples to the device and extract them from the list\n",
        "            inputs = batch[0].to(device)\n",
        "\n",
        "            # reinitialize lstm hidden state\n",
        "            hidden = model.init_hidden(inputs.size(0))\n",
        "            \n",
        "            # batch forward pass and predictions\n",
        "            logits, hidden = model(inputs, hidden)\n",
        "            preds[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE] = logits.argmax(dim=1)\n",
        "            \"\"\"\n",
        "            if batch_idx == 0:\n",
        "                print()\n",
        "                print(logits[:10])\n",
        "            \"\"\"\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95IVcdz7ASwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(classes, preds, labels, pourcents=False):\n",
        "    \"\"\"\n",
        "    Generates a confusion matrix indicated which class is often mixed up with which other class\n",
        "    :param classes: List of all possible classes in the model\n",
        "    :param preds: Classification predictions given by a model\n",
        "    :param labels: True labels for the given predictions\n",
        "    :param pourcents: Whether or not to express the values in the confusion matrix in percentages\n",
        "    :return: A m X m confusion matrix M where M(i,j) is how many times, on average, a point belonging to class i is\n",
        "    given class j\n",
        "    \"\"\"\n",
        "    m = len(classes)\n",
        "    conf_matrix = np.zeros((m, m))\n",
        "    for i, ground_truth in enumerate(classes):\n",
        "        class_idx = (labels == ground_truth)\n",
        "        for j, predicted in enumerate(classes):\n",
        "            if pourcents:\n",
        "                conf_matrix[i, j] = np.round(100 * np.mean(preds[class_idx] == predicted), 1)\n",
        "            else:\n",
        "                conf_matrix[i, j] = np.sum(preds[class_idx] == predicted)\n",
        "\n",
        "    return conf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL4Zxkznticx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load datasets\n",
        "training_data, training_labels = load_data('data_train_preprocessed.csv', folder=DATA_FOLDER, has_labels=True)\n",
        "submission_data = load_data('data_test_preprocessed.csv', folder=DATA_FOLDER)\n",
        "\n",
        "# extract classes\n",
        "classes = list(set(training_labels))\n",
        "classes.sort()\n",
        "num_classes = len(classes)\n",
        "\n",
        "print(num_classes)\n",
        "print()\n",
        "print(Counter(training_labels))\n",
        "print()\n",
        "print(len(training_data))\n",
        "print(len(training_labels))\n",
        "print(len(submission_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3IfpTi73ZjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "MAX_LENGTH = 512\n",
        "THRESHOLD = 5\n",
        "DATASET_FRACTION = 1  # fraction of training examples to use (between 0 and 1)\n",
        "\n",
        "USE_ONEHOT = True\n",
        "\n",
        "# set seeds for reproducibility\n",
        "SEED = 1234\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# split dataset\n",
        "train_data, train_labels, valid_data, valid_labels, train_idx_weights = \\\n",
        "    split_data(training_data, training_labels, valid_prop=.1, seed=SEED)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print()\n",
        "\n",
        "if USE_ONEHOT:\n",
        "    # make one hot encoding for train set labels\n",
        "    LABEL_SMOOTHING = .1\n",
        "    num_examples = train_labels.shape[0]\n",
        "\n",
        "    one_hot_helper = np.zeros((num_examples, num_classes))\n",
        "    one_hot_helper[np.arange(num_examples), train_labels] = 1\n",
        "    train_labels = one_hot_helper\n",
        "\n",
        "    train_labels[train_labels == 1] = 1 - LABEL_SMOOTHING\n",
        "    train_labels[train_labels == 0] = LABEL_SMOOTHING / (num_classes - 1)\n",
        "\n",
        "# tokenizer instantiation\n",
        "tokenizer = Tokenizer(train_data, THRESHOLD)\n",
        "print()\n",
        "print(len(tokenizer.tokens_to_ids))\n",
        "print()\n",
        "\n",
        "# prepare data for classification task\n",
        "train_dataloader = prepare_data(tokenizer, train_data, train_labels, train_idx_weights)\n",
        "valid_dataloader = prepare_data(tokenizer, valid_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o63-bKhCw21f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BloggerLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, fc_size, bidirectional, class_cnt):\n",
        "\n",
        "        super(BloggerLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.fc_size = fc_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.bidi_mult = 2 if bidirectional else 1\n",
        "\n",
        "        self.hidden = None\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size,\n",
        "                            hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        if fc_size is not None:\n",
        "            self.fc1 = nn.Linear(hidden_size * self.bidi_mult, fc_size)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(fc_size, class_cnt)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_size * self.bidi_mult, class_cnt)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers * self.bidi_mult, batch_size, self.hidden_size).zero_().to(device),\n",
        "                  weight.new(self.num_layers * self.bidi_mult, batch_size, self.hidden_size).zero_().to(device))\n",
        "        return hidden\n",
        "    \n",
        "    def forward(self, inputs, hidden):\n",
        "\n",
        "        embeddings = self.word_embeddings(inputs)\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "\n",
        "        out = output[:,-1]\n",
        "\n",
        "        if self.fc_size is not None:\n",
        "            out = self.fc1(out)\n",
        "            out = self.relu(out)\n",
        "            out = self.fc2(out)\n",
        "        else:\n",
        "            out = self.fc(out)\n",
        "\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyrmtnxPw24V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBED_SIZE = 400\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = .0\n",
        "FC_SIZE = 256\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "LEARNING_RATE = .00002\n",
        "WEIGHT_DECAY = .0005\n",
        "\n",
        "vocab_size = len(tokenizer.tokens_to_ids)\n",
        "\n",
        "# instantiate model\n",
        "model = BloggerLSTM(vocab_size, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, FC_SIZE, BIDIRECTIONAL, num_classes).to(device)\n",
        "\n",
        "# instantiate optimizer\n",
        "optimizer = optim.Adam(model.parameters())#, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# instantiate loss function\n",
        "if USE_ONEHOT:\n",
        "    criterion = cross_entropy_with_probs\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qwZ9wLhw2wK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # get tuple examples from the batch\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, labels = batch\n",
        "        \n",
        "        # reinitialize lstm hidden state\n",
        "        hidden = model.init_hidden(inputs.size(0))\n",
        "\n",
        "        # forward pass\n",
        "        logits, hidden = model(inputs, hidden)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # reset gradients and accumulate loss\n",
        "        optimizer.zero_grad()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # compute validation accuracy\n",
        "    preds = predict(model, valid_dataloader)\n",
        "    accuracy = np.mean(preds.numpy() == valid_labels)\n",
        "    \n",
        "    print(f'\\nEpoch {epoch+1}: loss={epoch_loss}')\n",
        "    print(f'Validation accuracy = {accuracy}\\n')\n",
        "\n",
        "    conf_matrix = confusion_matrix(classes, preds.numpy(), valid_labels, pourcents=True)\n",
        "    print(conf_matrix)\n",
        "    print()\n",
        "    \n",
        "    # save model if best accuracy yet\n",
        "    if accuracy > best_accuracy:\n",
        "        model_save_name = f'model_{LEARNING_RATE}_{MAX_LENGTH}_{BATCH_SIZE}_{NUM_EPOCHS}_{accuracy}.pt'\n",
        "        path = OUT_FOLDER + SAVED_MODELS_FOLDER + model_save_name\n",
        "        torch.save(model.state_dict(), path)\n",
        "        best_accuracy = accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUj-3t6spXeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BloggerLSTM(vocab_size, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, FC_SIZE, BIDIRECTIONAL, num_classes).to(device)\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "# compute validation accuracy\n",
        "preds = predict(model, valid_dataloader)\n",
        "accuracy = np.mean(preds.numpy() == valid_labels)\n",
        "\n",
        "print(f'\\nEpoch {epoch+1}: loss={epoch_loss}')\n",
        "print(f'Validation accuracy = {accuracy}\\n')\n",
        "\n",
        "conf_matrix = confusion_matrix(classes, preds.numpy(), valid_labels, pourcents=False)\n",
        "print(conf_matrix)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uJM7drxjVyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}